{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CouncilScribe\n",
    "## Automated City Council Meeting Transcription\n",
    "\n",
    "This notebook processes a city council meeting recording through a 6-stage pipeline:\n",
    "1. **Ingest** — Normalize audio to 16kHz mono WAV\n",
    "2. **Diarize** — Identify who spoke when (pyannote.audio)\n",
    "3. **Transcribe** — Speech-to-text with word timestamps (faster-whisper)\n",
    "4. **Identify** — Map speaker labels to real names\n",
    "5. **Enroll** — Save voice profiles for future meetings\n",
    "6. **Export** — Output Markdown, JSON, and SRT files\n",
    "\n",
    "Each stage checkpoints to Google Drive so you can resume after a session timeout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (pin numpy to avoid Colab compatibility issues)\n!pip install -q \"numpy<2.1\" && pip install -q faster-whisper pyannote.audio noisereduce soundfile pydub huggingface_hub llama-cpp-python scipy requests beautifulsoup4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or update CouncilScribe source\n",
    "import os\n",
    "import sys\n",
    "\n",
    "REPO_URL = \"https://github.com/chrisandrewsedu/CouncilScribe.git\"  # Update with your repo URL\n",
    "REPO_DIR = \"/content/CouncilScribe\"\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    !cd {REPO_DIR} && git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "\n",
    "# Add src to Python path\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "print(\"CouncilScribe source loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate Hugging Face (required for pyannote models)\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name} ({vram:.1f} GB VRAM)\")\nelse:\n    print(\"No GPU available. Will use CPU mode (slower, smaller model).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Meeting Configuration { display-mode: \"form\" }\n# @markdown Fill in the meeting details below:\n\nmeeting_id = \"2026-02-10-regular\"  # @param {type:\"string\"}\ncity = \"Bloomington\"  # @param {type:\"string\"}\nmeeting_date = \"2026-02-10\"  # @param {type:\"date\"}\nmeeting_type = \"Regular Session\"  # @param [\"Regular Session\", \"Special Session\", \"Work Session\", \"Committee Meeting\"]\n\n# @markdown ---\n# @markdown ### Audio Source\n# @markdown Choose how to provide the meeting recording:\naudio_source_type = \"URL\"  # @param [\"Local/Drive File\", \"URL\", \"CATS TV Browser\"]\n\n# @markdown **If Local/Drive File:** path to the audio/video file on Drive\naudio_file_path = \"/content/drive/MyDrive/CouncilScribe/meetings/input.mp4\"  # @param {type:\"string\"}\n\n# @markdown **If URL:** direct video URL or CATS TV page URL\naudio_url = \"https://catstv.net/government.php?issearch=govt&meeterid=117\"  # @param {type:\"string\"}\n\n# @markdown **If CATS TV Browser:** use the next cell to browse and select a meeting\n# @markdown ---\n\nnum_speakers_hint = 0  # @param {type:\"integer\"}\napply_noise_reduction = False  # @param {type:\"boolean\"}\nuse_llm_identification = True  # @param {type:\"boolean\"}\n\n# Set num_speakers to None if 0 (let pyannote auto-detect)\nnum_speakers = num_speakers_hint if num_speakers_hint > 0 else None\n\n# Resolve audio_path based on source type\nif audio_source_type == \"URL\":\n    audio_path = audio_url\n    print(f\"Audio source: URL\")\n    print(f\"  {audio_url}\")\nelif audio_source_type == \"CATS TV Browser\":\n    audio_path = None  # Will be set by the browser cell below\n    print(\"Audio source: CATS TV Browser (run the next cell to select a meeting)\")\nelse:\n    audio_path = audio_file_path\n    print(f\"Audio source: Local file\")\n    print(f\"  {audio_file_path}\")\n\nprint(f\"\\nMeeting: {city} {meeting_type} ({meeting_date})\")\nprint(f\"Speaker hint: {num_speakers or 'auto-detect'}\")"
  },
  {
   "cell_type": "code",
   "source": "# @title CATS TV Meeting Browser { display-mode: \"form\" }\n# @markdown Browse and select a meeting from the CATS TV archive.\n# @markdown Only needed if **Audio Source** is set to \"CATS TV Browser\" above.\n# @markdown\n# @markdown Adjust the search URL or limit as needed:\n\ncatstv_search_url = \"https://catstv.net/government.php?issearch=govt\"  # @param {type:\"string\"}\nresults_limit = 25  # @param {type:\"integer\"}\n\nfrom src.download import fetch_catstv_meetings, display_catstv_meetings\n\nprint(\"Fetching CATS TV meeting archive...\")\ncatstv_meetings = fetch_catstv_meetings(catstv_search_url)\nprint(f\"Found {len(catstv_meetings)} meetings.\\n\")\ndisplay_catstv_meetings(catstv_meetings, limit=results_limit)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Select a CATS TV Meeting { display-mode: \"form\" }\n# @markdown Enter the number from the list above to select a meeting:\n\nmeeting_number = 0  # @param {type:\"integer\"}\n\n# Guard against running this cell before the config/browser cells\nif \"catstv_meetings\" not in dir() or not catstv_meetings:\n    print(\"No meetings loaded. Run the 'CATS TV Meeting Browser' cell first.\")\nelif 0 <= meeting_number < len(catstv_meetings):\n    selected = catstv_meetings[meeting_number]\n    audio_path = selected[\"video_url\"]\n\n    # Auto-fill meeting metadata from CATS TV data\n    if selected[\"date\"]:\n        meeting_date = selected[\"date\"]\n    if selected[\"name\"]:\n        meeting_type = selected[\"name\"]\n        if selected[\"subtitle\"]:\n            meeting_type += f\" — {selected['subtitle']}\"\n\n    print(f\"Selected: {selected['name']}\")\n    if selected[\"subtitle\"]:\n        print(f\"  {selected['subtitle']}\")\n    print(f\"  Date: {selected['date']}\")\n    print(f\"  Duration: {selected['duration']}\")\n    print(f\"  Video URL: {audio_path}\")\n    if selected[\"documents_url\"]:\n        print(f\"  Documents: {selected['documents_url']}\")\nelse:\n    print(f\"Invalid selection: {meeting_number}. Must be 0-{len(catstv_meetings)-1}.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize pipeline state and directory structure\nfrom src.checkpoint import PipelineState, PipelineStage, ensure_drive_structure\nfrom src.models import Meeting, ProcessingMetadata\n\n# Defaults for variables that should have been set by config/browser cells\nif \"meeting_id\" not in dir() or not meeting_id:\n    meeting_id = \"unnamed-meeting\"\n    print(\"Warning: meeting_id not set. Run the Configuration cell first, or using default.\")\nif \"city\" not in dir():\n    city = \"Unknown\"\nif \"meeting_date\" not in dir():\n    meeting_date = \"\"\nif \"meeting_type\" not in dir():\n    meeting_type = \"Regular Session\"\nif \"audio_path\" not in dir() or not audio_path:\n    raise RuntimeError(\n        \"audio_path is not set. Run the Configuration cell and choose an audio source, \"\n        \"or use the CATS TV Browser to select a meeting first.\"\n    )\n\nmeeting_dir = ensure_drive_structure(meeting_id)\nstate = PipelineState(meeting_dir)\n\nmeeting = Meeting(\n    meeting_id=meeting_id,\n    city=city,\n    date=meeting_date,\n    meeting_type=meeting_type,\n    audio_source=audio_path,\n)\n\nprint(f\"Meeting directory: {meeting_dir}\")\nprint(f\"Pipeline state: stage {state.completed_stage.name}\")\nif state.completed_stage > PipelineStage.NOT_STARTED:\n    print(f\"  Resuming from checkpoint (stage {state.completed_stage.value}/6)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Stage 1 — Audio Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from src.ingest import normalize_audio\n",
    "\n",
    "wav_path = meeting_dir / \"audio.wav\"\n",
    "\n",
    "if state.is_complete(PipelineStage.INGESTED):\n",
    "    print(\"Stage 1 already complete. Skipping.\")\n",
    "    metadata = {\"duration_seconds\": 0}\n",
    "    # Recover duration from existing WAV\n",
    "    from src.audio_utils import get_audio_duration\n",
    "    metadata[\"duration_seconds\"] = get_audio_duration(wav_path)\n",
    "else:\n",
    "    print(\"Stage 1: Normalizing audio...\")\n",
    "    t0 = time.time()\n",
    "    metadata = normalize_audio(audio_path, wav_path, noise_reduce=apply_noise_reduction)\n",
    "    elapsed = time.time() - t0\n",
    "    state.mark_complete(PipelineStage.INGESTED)\n",
    "    print(f\"  Done in {elapsed:.1f}s\")\n",
    "\n",
    "meeting.duration_seconds = metadata[\"duration_seconds\"]\n",
    "duration_min = meeting.duration_seconds / 60\n",
    "print(f\"  Audio duration: {duration_min:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Stage 2 — Speaker Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom src.diarize import load_diarization_pipeline, run_diarization, extract_speaker_embeddings\nfrom src.models import Segment\n\ndiarization_path = meeting_dir / \"diarization.json\"\nembeddings_path = meeting_dir / \"embeddings.json\"\n\nif state.is_complete(PipelineStage.DIARIZED):\n    print(\"Stage 2 already complete. Loading from checkpoint...\")\n    with open(diarization_path, \"r\") as f:\n        segments = [Segment.from_dict(d) for d in json.load(f)]\n    print(f\"  Loaded {len(segments)} segments\")\nelse:\n    print(\"Stage 2: Running speaker diarization (progress bar below)...\")\n    # Get HF token from environment (set by notebook_login)\n    from huggingface_hub import get_token\n    hf_token = get_token()\n\n    t0 = time.time()\n    pipeline = load_diarization_pipeline(hf_token)\n    segments = run_diarization(pipeline, wav_path, num_speakers=num_speakers)\n    elapsed = time.time() - t0\n\n    # Save checkpoint\n    with open(diarization_path, \"w\") as f:\n        json.dump([s.to_dict() for s in segments], f, indent=2)\n\n    # Extract speaker embeddings\n    print(\"  Extracting speaker embeddings...\")\n    speaker_embeddings = extract_speaker_embeddings(wav_path, segments, hf_token)\n\n    # Save embeddings as lists for JSON serialization\n    emb_data = {k: v.tolist() for k, v in speaker_embeddings.items()}\n    with open(embeddings_path, \"w\") as f:\n        json.dump(emb_data, f)\n\n    # Free GPU memory\n    del pipeline\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    state.mark_complete(PipelineStage.DIARIZED)\n    print(f\"  Done in {elapsed:.1f}s\")\n\nunique_speakers = set(s.speaker_label for s in segments)\nprint(f\"  {len(segments)} segments, {len(unique_speakers)} speakers detected\")\nmeeting.processing_metadata.diarization_model = \"pyannote/speaker-diarization-3.1\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Stage 3 — Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transcribe import load_whisper_model, transcribe_segments, save_raw_transcript, load_raw_transcript\n",
    "from src import config\n",
    "\n",
    "transcript_path = meeting_dir / \"transcript_raw.json\"\n",
    "\n",
    "if state.is_complete(PipelineStage.TRANSCRIBED):\n",
    "    print(\"Stage 3 already complete. Loading from checkpoint...\")\n",
    "    segments = load_raw_transcript(transcript_path)\n",
    "    print(f\"  Loaded {len(segments)} transcribed segments\")\n",
    "else:\n",
    "    print(\"Stage 3: Transcribing segments...\")\n",
    "    resume_from = state.transcription_progress\n",
    "    if resume_from > 0:\n",
    "        print(f\"  Resuming from segment {resume_from}/{len(segments)}\")\n",
    "        # Load partially transcribed segments\n",
    "        if transcript_path.exists():\n",
    "            segments = load_raw_transcript(transcript_path)\n",
    "\n",
    "    t0 = time.time()\n",
    "    whisper_model = load_whisper_model()\n",
    "\n",
    "    model_name = config.WHISPER_MODEL_GPU if torch.cuda.is_available() else config.WHISPER_MODEL_CPU\n",
    "    meeting.processing_metadata.transcription_model = model_name\n",
    "    meeting.processing_metadata.gpu_used = torch.cuda.is_available()\n",
    "    print(f\"  Using model: {model_name}\")\n",
    "\n",
    "    def checkpoint_fn(current, total):\n",
    "        save_raw_transcript(segments, transcript_path)\n",
    "        state.update_transcription_progress(current, total)\n",
    "        pct = (current / total) * 100\n",
    "        print(f\"  Checkpoint: {current}/{total} segments ({pct:.0f}%)\")\n",
    "\n",
    "    segments = transcribe_segments(\n",
    "        whisper_model, wav_path, segments,\n",
    "        checkpoint_callback=checkpoint_fn,\n",
    "        resume_from=resume_from,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Save final transcript\n",
    "    save_raw_transcript(segments, transcript_path)\n",
    "\n",
    "    # Free GPU memory\n",
    "    del whisper_model\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    state.mark_complete(PipelineStage.TRANSCRIBED)\n",
    "    print(f\"  Done in {elapsed:.1f}s\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n  Sample transcript:\")\n",
    "for seg in segments[:5]:\n",
    "    if seg.text:\n",
    "        print(f\"    [{seg.speaker_label}] {seg.text[:80]}...\" if len(seg.text) > 80 else f\"    [{seg.speaker_label}] {seg.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Stage 4 — Speaker Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.identify import identify_speakers, apply_mappings_to_segments, flag_for_review\n",
    "from src.enroll import load_profiles, get_stored_centroids\n",
    "from src.models import SpeakerMapping\n",
    "\n",
    "named_transcript_path = meeting_dir / \"transcript_named.json\"\n",
    "\n",
    "if state.is_complete(PipelineStage.IDENTIFIED):\n",
    "    print(\"Stage 4 already complete. Loading from checkpoint...\")\n",
    "    with open(named_transcript_path, \"r\") as f:\n",
    "        meeting_data = json.load(f)\n",
    "    meeting = Meeting.from_dict(meeting_data)\n",
    "    segments = meeting.segments\n",
    "else:\n",
    "    print(\"Stage 4: Identifying speakers...\")\n",
    "\n",
    "    # Load speaker embeddings\n",
    "    if embeddings_path.exists():\n",
    "        with open(embeddings_path, \"r\") as f:\n",
    "            emb_data = json.load(f)\n",
    "        speaker_embeddings = {k: np.array(v) for k, v in emb_data.items()}\n",
    "    else:\n",
    "        speaker_embeddings = {}\n",
    "\n",
    "    # Load existing voice profiles for Layer 1\n",
    "    profile_db = load_profiles()\n",
    "    stored_centroids = get_stored_centroids(profile_db)\n",
    "    if stored_centroids:\n",
    "        print(f\"  Loaded {len(stored_centroids)} voice profiles\")\n",
    "\n",
    "    # Layer 3: LLM (optional)\n",
    "    llm_fn = None\n",
    "    if use_llm_identification:\n",
    "        print(\"  Loading LLM for speaker identification...\")\n",
    "        from src.llm_utils import load_llm, llm_identify_speakers, unload_llm\n",
    "        llm = load_llm()\n",
    "        llm_fn = lambda segs, maps: llm_identify_speakers(llm, segs, maps)\n",
    "\n",
    "    # Run identification\n",
    "    t0 = time.time()\n",
    "    mappings = identify_speakers(\n",
    "        segments, speaker_embeddings,\n",
    "        stored_profiles=stored_centroids if stored_centroids else None,\n",
    "        llm_identify_fn=llm_fn,\n",
    "    )\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    # Free LLM memory\n",
    "    if use_llm_identification:\n",
    "        unload_llm(llm)\n",
    "        del llm\n",
    "\n",
    "    # Apply to segments\n",
    "    segments = apply_mappings_to_segments(segments, mappings)\n",
    "    meeting.segments = segments\n",
    "    meeting.speakers = mappings\n",
    "\n",
    "    print(f\"  Done in {elapsed:.1f}s\")\n",
    "\n",
    "    # Show results\n",
    "    for label, m in mappings.items():\n",
    "        status = \"REVIEW\" if m.needs_review else \"OK\"\n",
    "        name = m.speaker_name or \"(unidentified)\"\n",
    "        print(f\"    {label} -> {name} (conf={m.confidence:.2f}, method={m.id_method}, {status})\")\n",
    "\n",
    "    review_needed = flag_for_review(mappings)\n",
    "    if review_needed:\n",
    "        print(f\"\\n  {len(review_needed)} speaker(s) need human review (see next cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Human Review (Optional) { display-mode: \"form\" }\n",
    "# @markdown Run this cell to manually correct speaker identifications.\n",
    "# @markdown Leave blank to skip. Format: SPEAKER_00=Mayor Johnson\n",
    "\n",
    "corrections_text = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "if corrections_text.strip():\n",
    "    for pair in corrections_text.split(\",\"):\n",
    "        pair = pair.strip()\n",
    "        if \"=\" in pair:\n",
    "            label, name = pair.split(\"=\", 1)\n",
    "            label = label.strip()\n",
    "            name = name.strip()\n",
    "            if label in meeting.speakers:\n",
    "                meeting.speakers[label].speaker_name = name\n",
    "                meeting.speakers[label].confidence = 1.0\n",
    "                meeting.speakers[label].id_method = \"human_review\"\n",
    "                meeting.speakers[label].needs_review = False\n",
    "                print(f\"  Updated: {label} -> {name}\")\n",
    "\n",
    "    # Re-apply mappings\n",
    "    segments = apply_mappings_to_segments(segments, meeting.speakers)\n",
    "    meeting.segments = segments\n",
    "    print(\"  Corrections applied.\")\n",
    "else:\n",
    "    print(\"  No corrections. Continuing.\")\n",
    "\n",
    "# Save named transcript checkpoint\n",
    "with open(named_transcript_path, \"w\") as f:\n",
    "    json.dump(meeting.to_dict(), f, indent=2)\n",
    "state.mark_complete(PipelineStage.IDENTIFIED)\n",
    "print(\"  Stage 4 checkpoint saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Stage 5 — Voice Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.enroll import load_profiles, save_profiles, enroll_speakers\n",
    "\n",
    "if state.is_complete(PipelineStage.ENROLLED):\n",
    "    print(\"Stage 5 already complete. Skipping.\")\n",
    "else:\n",
    "    print(\"Stage 5: Enrolling voice profiles...\")\n",
    "\n",
    "    # Load embeddings\n",
    "    if embeddings_path.exists():\n",
    "        with open(embeddings_path, \"r\") as f:\n",
    "            emb_data = json.load(f)\n",
    "        speaker_embeddings = {k: np.array(v) for k, v in emb_data.items()}\n",
    "    else:\n",
    "        speaker_embeddings = {}\n",
    "\n",
    "    profile_db = load_profiles()\n",
    "    before_count = len(profile_db.profiles)\n",
    "\n",
    "    profile_db = enroll_speakers(\n",
    "        profile_db, speaker_embeddings, meeting.speakers,\n",
    "        meeting_id=meeting_id, segments=segments,\n",
    "    )\n",
    "\n",
    "    save_profiles(profile_db)\n",
    "    after_count = len(profile_db.profiles)\n",
    "    new_profiles = after_count - before_count\n",
    "\n",
    "    state.mark_complete(PipelineStage.ENROLLED)\n",
    "    print(f\"  Enrolled {new_profiles} new profile(s). Total: {after_count}\")\n",
    "    for pid, p in profile_db.profiles.items():\n",
    "        print(f\"    {pid}: {p.display_name} ({len(p.meetings_seen)} meetings, {p.total_segments_confirmed} segments)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Stage 6 — Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.export import export_all\n",
    "\n",
    "if state.is_complete(PipelineStage.EXPORTED):\n",
    "    print(\"Stage 6 already complete.\")\n",
    "else:\n",
    "    print(\"Stage 6: Exporting transcript...\")\n",
    "\n",
    "    export_dir = meeting_dir / \"exports\"\n",
    "    results = export_all(meeting, export_dir)\n",
    "\n",
    "    state.mark_complete(PipelineStage.EXPORTED)\n",
    "    print(\"  Export complete:\")\n",
    "    for fmt, path in results.items():\n",
    "        print(f\"    {fmt}: {path}\")\n",
    "\n",
    "print(\"\\nPipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Preview Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the Markdown transcript\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "md_path = meeting_dir / \"exports\" / \"transcript.md\"\n",
    "if md_path.exists():\n",
    "    content = md_path.read_text()\n",
    "    # Show first 3000 chars\n",
    "    preview = content[:3000]\n",
    "    if len(content) > 3000:\n",
    "        preview += f\"\\n\\n*... ({len(content) - 3000} more characters)*\"\n",
    "    display(Markdown(preview))\n",
    "else:\n",
    "    print(\"No transcript found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Profile Manager { display-mode: \"form\" }\n",
    "# @markdown View or manage stored voice profiles.\n",
    "\n",
    "action = \"list\"  # @param [\"list\", \"delete\"]\n",
    "profile_to_delete = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "from src.enroll import load_profiles, save_profiles\n",
    "\n",
    "db = load_profiles()\n",
    "\n",
    "if action == \"list\":\n",
    "    if not db.profiles:\n",
    "        print(\"No profiles stored yet.\")\n",
    "    else:\n",
    "        print(f\"Stored profiles ({len(db.profiles)}):\")\n",
    "        for pid, p in db.profiles.items():\n",
    "            print(f\"  {pid}: {p.display_name}\")\n",
    "            print(f\"    Meetings: {', '.join(p.meetings_seen)}\")\n",
    "            print(f\"    Confirmed segments: {p.total_segments_confirmed}\")\n",
    "            print(f\"    Embeddings: {len(p.embeddings)}\")\n",
    "\n",
    "elif action == \"delete\" and profile_to_delete:\n",
    "    if profile_to_delete in db.profiles:\n",
    "        del db.profiles[profile_to_delete]\n",
    "        save_profiles(db)\n",
    "        print(f\"Deleted profile: {profile_to_delete}\")\n",
    "    else:\n",
    "        print(f\"Profile not found: {profile_to_delete}\")\n",
    "        print(f\"Available: {', '.join(db.profiles.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Batch Processing { display-mode: \"form\" }\n",
    "# @markdown Process multiple meetings from a folder.\n",
    "# @markdown Audio files should be in: /content/drive/MyDrive/CouncilScribe/meetings/batch/\n",
    "\n",
    "batch_city = \"Springfield\"  # @param {type:\"string\"}\n",
    "batch_folder = \"/content/drive/MyDrive/CouncilScribe/meetings/batch\"  # @param {type:\"string\"}\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "batch_path = Path(batch_folder)\n",
    "if batch_path.exists():\n",
    "    audio_extensions = {\".mp4\", \".mkv\", \".wav\", \".mp3\", \".m4a\", \".ogg\", \".flac\"}\n",
    "    files = sorted([f for f in batch_path.iterdir() if f.suffix.lower() in audio_extensions])\n",
    "    print(f\"Found {len(files)} audio files:\")\n",
    "    for f in files:\n",
    "        print(f\"  {f.name}\")\n",
    "    print(\"\\nTo process these, update the Configuration cell for each file and run the pipeline.\")\n",
    "else:\n",
    "    print(f\"Batch folder not found: {batch_folder}\")\n",
    "    print(\"Create it and add audio files to use batch processing.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}